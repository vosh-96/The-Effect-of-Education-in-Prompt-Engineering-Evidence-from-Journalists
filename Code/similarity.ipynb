{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xlsx file\n",
    "raw_df = pd.read_excel('../Data/clean_ExpertAnnotated_survey_data_AI4Journalist.xlsx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/53/xr64_z814jlcgr3vjrwr7yb80000gn/T/ipykernel_50196/4043256980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['writing_a'] = df['1_writing_a'].fillna('') + df['2_writing_a'].fillna('')\n",
      "/var/folders/53/xr64_z814jlcgr3vjrwr7yb80000gn/T/ipykernel_50196/4043256980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['writing_b'] = df['1_writing_b'].fillna('') + df['2_writing_b'].fillna('')\n"
     ]
    }
   ],
   "source": [
    "# extract the columns: group, ResponseId, Finished, 1_writing_a, 1_writing_b, 2_writing_a, 2_writing_b\n",
    "# combine 1_writing_a and 2_writing_a into one column, combone 1_writing_b and 2_writing_b into one column\n",
    "# rename the columns to: group, ResponseId, Finished, writing_a, writing_b\n",
    "\n",
    "df = raw_df[['group', 'ResponseId', 'Finished', '1_writing_a', '1_writing_b', '2_writing_a', '2_writing_b']]\n",
    "df['writing_a'] = df['1_writing_a'].fillna('') + df['2_writing_a'].fillna('')\n",
    "df['writing_b'] = df['1_writing_b'].fillna('') + df['2_writing_b'].fillna('')\n",
    "df = df[['group', 'ResponseId', 'Finished', 'writing_a', 'writing_b']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['writing_a_clean_lst'] = df['writing_a'].str.lower()\n",
    "df['writing_a_clean_lst'] = df['writing_a_clean_lst'].apply(remove_punctuation)\n",
    "df['writing_a_clean_lst'] = df['writing_a_clean_lst'].str.split()\n",
    "\n",
    "df['writing_b_clean_lst'] = df['writing_b'].str.lower()\n",
    "df['writing_b_clean_lst'] = df['writing_b_clean_lst'].apply(remove_punctuation)\n",
    "df['writing_b_clean_lst'] = df['writing_b_clean_lst'].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Within Subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the pre-trained word vectors from Google News\n",
    "\n",
    "# load the pre-trained word vectors\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "def get_cosine_similarity(writing_a_lst: list, writing_b_lst: list):\n",
    "    if not writing_a_lst or not writing_b_lst:\n",
    "        return 0\n",
    "    else:\n",
    "        words_a = [word for word in writing_a_lst if word in word2vec_model.key_to_index]\n",
    "        words_b = [word for word in writing_b_lst if word in word2vec_model.key_to_index]\n",
    "        if len(words_a) == 0 or len(words_b) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            vector_a = np.mean([word2vec_model[word] for word in words_a], axis=0)\n",
    "            vector_b = np.mean([word2vec_model[word] for word in words_b], axis=0)\n",
    "            norm_a = np.linalg.norm(vector_a)\n",
    "            norm_b = np.linalg.norm(vector_b)\n",
    "            if norm_a == 0 or norm_b == 0:\n",
    "                return 0\n",
    "            return np.dot(vector_a, vector_b) / (norm_a * norm_b)\n",
    "        \n",
    "def get_unnormalized_dot_product(writing_a_lst: list, writing_b_lst: list):\n",
    "    if not writing_a_lst or not writing_b_lst:\n",
    "        return 0\n",
    "    else:\n",
    "        words_a = [word for word in writing_a_lst if word in word2vec_model.key_to_index]\n",
    "        words_b = [word for word in writing_b_lst if word in word2vec_model.key_to_index]\n",
    "        if len(words_a) == 0 or len(words_b) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            vector_a = np.mean([word2vec_model[word] for word in words_a], axis=0)\n",
    "            vector_b = np.mean([word2vec_model[word] for word in words_b], axis=0)\n",
    "            return np.dot(vector_a, vector_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cosine_similarity_w2v'] = df.apply(lambda row: get_cosine_similarity(row['writing_a_clean_lst'], row['writing_b_clean_lst']), axis=1)\n",
    "df['unnormalized_dot_product_w2v'] = df.apply(lambda row: get_unnormalized_dot_product(row['writing_a_clean_lst'], row['writing_b_clean_lst']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_openai_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   if text is None or text == \"\":\n",
    "        return 0\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the OpenAI embeddings for writing_a and writing_b\n",
    "# least pre-processing, so use raw texts\n",
    "df['openai_embedding_a'] = df['writing_a'].apply(get_openai_embedding)\n",
    "df['openai_embedding_b'] = df['writing_b'].apply(get_openai_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cosine_similarity_openai(writing_a: str, writing_b: str):\n",
    "#     embedding_a = get_openai_embedding(writing_a)\n",
    "#     embedding_b = get_openai_embedding(writing_b)\n",
    "#     norm_a = np.linalg.norm(embedding_a)\n",
    "#     norm_b = np.linalg.norm(embedding_b)\n",
    "#     if norm_a == 0 or norm_b == 0:\n",
    "#         return 0\n",
    "#     return np.dot(embedding_a, embedding_b) / (norm_a * norm_b)\n",
    "\n",
    "# def get_unnormalized_dot_product_openai(writing_a: str, writing_b: str):\n",
    "#     embedding_a = get_openai_embedding(writing_a)\n",
    "#     embedding_b = get_openai_embedding(writing_b)\n",
    "#     return np.dot(embedding_a, embedding_b)\n",
    "\n",
    "def get_cosine_similarity_openai(embedding_a, embedding_b):\n",
    "    norm_a = np.linalg.norm(embedding_a)\n",
    "    norm_b = np.linalg.norm(embedding_b)\n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0\n",
    "    return np.dot(embedding_a, embedding_b) / (norm_a * norm_b)\n",
    "\n",
    "def get_unnormalized_dot_product_openai(embedding_a, embedding_b):\n",
    "    return np.dot(embedding_a, embedding_b)\n",
    "\n",
    "def get_euclidean_distance_openai(embedding_a, embedding_b):\n",
    "    return np.linalg.norm(np.array(embedding_a) - np.array(embedding_b))\n",
    "\n",
    "# openai embedding output is normalized, so cosine similarity is the dot product of the embeddings\n",
    "df[\"cosine_similarity_openai\"] = df.apply(lambda row: get_cosine_similarity_openai(row['openai_embedding_a'], row['openai_embedding_b']), axis=1)\n",
    "# df[\"unnormalized_dot_product_openai\"] = df.apply(lambda row: get_unnormalized_dot_product_openai(row['openai_embedding_a'], row['openai_embedding_b']), axis=1)\n",
    "df[\"euclidean_distance_openai\"] = df.apply(lambda row: get_euclidean_distance_openai(row['openai_embedding_a'], row['openai_embedding_b']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.matutils import cossim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Combine all words from both columns for dictionary creation\n",
    "all_words = df['writing_a_clean_lst'].tolist() + df['writing_b_clean_lst'].tolist()\n",
    "\n",
    "# Flatten the list of lists\n",
    "all_words_flat = [word for sublist in all_words for word in sublist]\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary([all_words_flat])\n",
    "corpus = [dictionary.doc2bow(text) for text in df['writing_a_clean_lst'].tolist() + df['writing_b_clean_lst'].tolist()]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=2, id2word=dictionary)\n",
    "\n",
    "# Function to get LDA vector\n",
    "def get_lda_vector(word_lst: list):\n",
    "    bow = dictionary.doc2bow(word_lst)\n",
    "    lda_vector = lda_model[bow]\n",
    "    return lda_vector\n",
    "\n",
    "# Similarity calculation function using LDA vectors\n",
    "def get_cosine_similarity_lda(writing_a_lst, writing_b_lst):\n",
    "    if not writing_a_lst or not writing_b_lst:\n",
    "        return 0\n",
    "    vector_a = get_lda_vector(writing_a_lst)\n",
    "    vector_b = get_lda_vector(writing_b_lst)\n",
    "    return cossim(vector_a, vector_b)\n",
    "\n",
    "# def get_unnormalized_dot_product_lda(writing_a_lst, writing_b_lst):\n",
    "#     if not writing_a_lst or not writing_b_lst:\n",
    "#         return 0\n",
    "#     vector_a = get_lda_vector(writing_a_lst)\n",
    "#     vector_b = get_lda_vector(writing_b_lst)\n",
    "#     return np.dot(vector_a, vector_b)\n",
    "\n",
    "# Apply the similarity function to each row of the DataFrame\n",
    "df['lda_cosine_similarity'] = df.apply(lambda row: get_cosine_similarity_lda(row['writing_a_clean_lst'], row['writing_b_clean_lst']), axis=1)\n",
    "# df['lda_unnormalized_dot_product'] = df.apply(lambda row: get_unnormalized_dot_product_lda(row['writing_a_clean_lst'], row['writing_b_clean_lst']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('../Data/within_subject_similarity.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Between Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df = pd.read_excel('../Data/within_subject_similarity.xlsx')\n",
    "similarity_df['writing_a_clean_lst'] = similarity_df.writing_a_clean_lst.apply(ast.literal_eval)\n",
    "similarity_df['writing_b_clean_lst'] = similarity_df.writing_b_clean_lst.apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_annotated_df = pd.read_csv('../Data/annotated_data_240520.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df and expert_evaluated_df\n",
    "# drop rows with NA journalist value\n",
    "merged_df = pd.merge(similarity_df, expert_annotated_df[['ResponseId', 'journalist']], on='ResponseId', how='inner').dropna(subset=['journalist']).reset_index(drop=True)\n",
    "merged_df.journalist = merged_df.journalist.apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>ResponseId</th>\n",
       "      <th>Finished</th>\n",
       "      <th>writing_a</th>\n",
       "      <th>writing_b</th>\n",
       "      <th>writing_a_clean_lst</th>\n",
       "      <th>writing_b_clean_lst</th>\n",
       "      <th>cosine_similarity_w2v</th>\n",
       "      <th>unnormalized_dot_product_w2v</th>\n",
       "      <th>openai_embedding_a</th>\n",
       "      <th>openai_embedding_b</th>\n",
       "      <th>cosine_similarity_openai</th>\n",
       "      <th>euclidean_distance_openai</th>\n",
       "      <th>lda_cosine_similarity</th>\n",
       "      <th>journalist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>R_4dnZuujLN4Mn9um</td>\n",
       "      <td>True</td>\n",
       "      <td>\"New study reveals complex impact of psychiatr...</td>\n",
       "      <td>\\n\"Study finds alarming link between gun viole...</td>\n",
       "      <td>[new, study, reveals, complex, impact, of, psy...</td>\n",
       "      <td>[study, finds, alarming, link, between, gun, v...</td>\n",
       "      <td>0.810496</td>\n",
       "      <td>0.713841</td>\n",
       "      <td>[-0.025056514889001846, -0.03770783916115761, ...</td>\n",
       "      <td>[0.023708993569016457, 0.02937544323503971, 0....</td>\n",
       "      <td>0.448127</td>\n",
       "      <td>1.050593</td>\n",
       "      <td>0.524188</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>R_6G8TRK7mezCmvlE</td>\n",
       "      <td>True</td>\n",
       "      <td>üß†üí° New Insights in Managing Suicide Risk! Rece...</td>\n",
       "      <td>üö® New study reveals a stark reality: Exposure ...</td>\n",
       "      <td>[üß†üí°, new, insights, in, managing, suicide, ris...</td>\n",
       "      <td>[üö®, new, study, reveals, a, stark, reality, ex...</td>\n",
       "      <td>0.825563</td>\n",
       "      <td>0.754296</td>\n",
       "      <td>[-0.018861589953303337, -0.020371491089463234,...</td>\n",
       "      <td>[0.020731830969452858, 0.016379600390791893, 0...</td>\n",
       "      <td>0.549861</td>\n",
       "      <td>0.948830</td>\n",
       "      <td>0.038555</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>R_7GUlxEwsm4ANY9n</td>\n",
       "      <td>True</td>\n",
       "      <td>\"New study finds psychiatric hospitalization r...</td>\n",
       "      <td>A Study on 3015 Black adults shows GVE signifi...</td>\n",
       "      <td>[new, study, finds, psychiatric, hospitalizati...</td>\n",
       "      <td>[a, study, on, 3015, black, adults, shows, gve...</td>\n",
       "      <td>0.740004</td>\n",
       "      <td>0.790497</td>\n",
       "      <td>[-0.015813207253813744, -0.028458785265684128,...</td>\n",
       "      <td>[0.04760603606700897, 0.021079791709780693, 0....</td>\n",
       "      <td>0.412520</td>\n",
       "      <td>1.083956</td>\n",
       "      <td>0.043443</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>R_2fPKnwwJ3SXlNdN</td>\n",
       "      <td>True</td>\n",
       "      <td>Psychiatric hospitalization reduces suicide ri...</td>\n",
       "      <td>New study by @AMAJournal reveals alarming asso...</td>\n",
       "      <td>[psychiatric, hospitalization, reduces, suicid...</td>\n",
       "      <td>[new, study, by, amajournal, reveals, alarming...</td>\n",
       "      <td>0.783746</td>\n",
       "      <td>0.683887</td>\n",
       "      <td>[-0.006001750472933054, -0.028704537078738213,...</td>\n",
       "      <td>[0.025258364155888557, 0.012829821556806564, 0...</td>\n",
       "      <td>0.507656</td>\n",
       "      <td>0.992314</td>\n",
       "      <td>0.047468</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>R_2saDzTZSaCPpZNs</td>\n",
       "      <td>True</td>\n",
       "      <td>\"New study in @JAMAPsych shows psychiatric hos...</td>\n",
       "      <td>\"New research suggests that reducing gun viole...</td>\n",
       "      <td>[new, study, in, jamapsych, shows, psychiatric...</td>\n",
       "      <td>[new, research, suggests, that, reducing, gun,...</td>\n",
       "      <td>0.682288</td>\n",
       "      <td>0.733727</td>\n",
       "      <td>[-0.02892470918595791, -0.03582962229847908, 0...</td>\n",
       "      <td>[0.054963596165180206, 0.011303708888590336, 0...</td>\n",
       "      <td>0.522809</td>\n",
       "      <td>0.976924</td>\n",
       "      <td>0.750716</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>R_2g7nDxYeFPr352v</td>\n",
       "      <td>True</td>\n",
       "      <td>Unraveling the puzzle of psychiatric hospitali...</td>\n",
       "      <td>\"Research shows stark connections: Gun violenc...</td>\n",
       "      <td>[unraveling, the, puzzle, of, psychiatric, hos...</td>\n",
       "      <td>[research, shows, stark, connections, gun, vio...</td>\n",
       "      <td>0.809810</td>\n",
       "      <td>0.757519</td>\n",
       "      <td>[-0.0008465779246762395, -0.024789869785308838...</td>\n",
       "      <td>[0.022167474031448364, 0.019238609820604324, 0...</td>\n",
       "      <td>0.516120</td>\n",
       "      <td>0.983748</td>\n",
       "      <td>0.249582</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>R_63y8AAf8TLPGRO3</td>\n",
       "      <td>True</td>\n",
       "      <td>New study reveals: Psychiatric hospitalization...</td>\n",
       "      <td>\"New study reveals a strong link between gun v...</td>\n",
       "      <td>[new, study, reveals, psychiatric, hospitaliza...</td>\n",
       "      <td>[new, study, reveals, a, strong, link, between...</td>\n",
       "      <td>0.772005</td>\n",
       "      <td>0.815978</td>\n",
       "      <td>[-0.025526609271764755, -0.01785901188850403, ...</td>\n",
       "      <td>[0.021031426265835762, 0.018929390236735344, 0...</td>\n",
       "      <td>0.551211</td>\n",
       "      <td>0.947406</td>\n",
       "      <td>0.047959</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>R_2Pe7Wu3hNtg2ZU6</td>\n",
       "      <td>True</td>\n",
       "      <td>üîç New study finds psychiatric hospitalization ...</td>\n",
       "      <td>üî´üß† Study finds Black adults exposed to gun vio...</td>\n",
       "      <td>[üîç, new, study, finds, psychiatric, hospitaliz...</td>\n",
       "      <td>[üî´üß†, study, finds, black, adults, exposed, to,...</td>\n",
       "      <td>0.731112</td>\n",
       "      <td>0.827849</td>\n",
       "      <td>[-0.042180195450782776, -0.018665548413991928,...</td>\n",
       "      <td>[0.03014543652534485, 0.04210395738482475, 0.0...</td>\n",
       "      <td>0.481462</td>\n",
       "      <td>1.018370</td>\n",
       "      <td>0.998329</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>R_4Ld8n99yqQacpmu</td>\n",
       "      <td>True</td>\n",
       "      <td>\"üí° Think psychiatric hospitalization's the ult...</td>\n",
       "      <td>\"New study finds alarming link between gun vio...</td>\n",
       "      <td>[üí°, think, psychiatric, hospitalizations, the,...</td>\n",
       "      <td>[new, study, finds, alarming, link, between, g...</td>\n",
       "      <td>0.809953</td>\n",
       "      <td>0.653381</td>\n",
       "      <td>[-0.011265125125646591, -0.04513534903526306, ...</td>\n",
       "      <td>[0.0172509104013443, 0.037992093712091446, 0.0...</td>\n",
       "      <td>0.442072</td>\n",
       "      <td>1.056341</td>\n",
       "      <td>0.041147</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>R_7EnJjQXJawFTRwm</td>\n",
       "      <td>True</td>\n",
       "      <td>New study shows psychiatric hospitalization re...</td>\n",
       "      <td>\"Study reveals Black individuals exposed to gu...</td>\n",
       "      <td>[new, study, shows, psychiatric, hospitalizati...</td>\n",
       "      <td>[study, reveals, black, individuals, exposed, ...</td>\n",
       "      <td>0.772831</td>\n",
       "      <td>0.963211</td>\n",
       "      <td>[-0.01255673449486494, -0.03516360744833946, 0...</td>\n",
       "      <td>[0.02446923218667507, 0.013484450988471508, 0....</td>\n",
       "      <td>0.570545</td>\n",
       "      <td>0.926774</td>\n",
       "      <td>0.999906</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>R_7uPaeBlCYnge9ax</td>\n",
       "      <td>True</td>\n",
       "      <td>üåü Our latest research results provide smarter,...</td>\n",
       "      <td>\"üí° New study reveals a strong link between gun...</td>\n",
       "      <td>[üåü, our, latest, research, results, provide, s...</td>\n",
       "      <td>[üí°, new, study, reveals, a, strong, link, betw...</td>\n",
       "      <td>0.727985</td>\n",
       "      <td>0.809747</td>\n",
       "      <td>[0.027828657999634743, -0.01811208575963974, 0...</td>\n",
       "      <td>[0.0053543285466730595, 0.011121646501123905, ...</td>\n",
       "      <td>0.392544</td>\n",
       "      <td>1.102231</td>\n",
       "      <td>0.069651</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>R_2tnxsmidy0Q9YQ1</td>\n",
       "      <td>True</td>\n",
       "      <td>A study examined psychiatric hospitalization's...</td>\n",
       "      <td>\\nNew study reveals alarming link between gun ...</td>\n",
       "      <td>[a, study, examined, psychiatric, hospitalizat...</td>\n",
       "      <td>[new, study, reveals, alarming, link, between,...</td>\n",
       "      <td>0.741933</td>\n",
       "      <td>0.731070</td>\n",
       "      <td>[-0.01983460783958435, -0.05631333217024803, 0...</td>\n",
       "      <td>[0.01820780709385872, 0.02473226934671402, -4....</td>\n",
       "      <td>0.385038</td>\n",
       "      <td>1.109019</td>\n",
       "      <td>0.954322</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>R_7sdyQIA4VE7WPTo</td>\n",
       "      <td>True</td>\n",
       "      <td>Hey there, curious minds! üåü Discover how 'brai...</td>\n",
       "      <td>Hey politicians, wake up! Black Americans face...</td>\n",
       "      <td>[hey, there, curious, minds, üåü, discover, how,...</td>\n",
       "      <td>[hey, politicians, wake, up, black, americans,...</td>\n",
       "      <td>0.695317</td>\n",
       "      <td>0.639158</td>\n",
       "      <td>[0.005720171611756086, -0.020700884982943535, ...</td>\n",
       "      <td>[0.05163086950778961, 0.02711629867553711, 0.0...</td>\n",
       "      <td>0.313337</td>\n",
       "      <td>1.171890</td>\n",
       "      <td>0.999935</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>R_7IfLndj9JNO63Vw</td>\n",
       "      <td>True</td>\n",
       "      <td>üìä Unlock insights into psychiatric hospitaliza...</td>\n",
       "      <td>üíî Heartbreaking findings: Black individuals fa...</td>\n",
       "      <td>[üìä, unlock, insights, into, psychiatric, hospi...</td>\n",
       "      <td>[üíî, heartbreaking, findings, black, individual...</td>\n",
       "      <td>0.673836</td>\n",
       "      <td>0.750695</td>\n",
       "      <td>[-0.015350842848420143, -0.04711201786994934, ...</td>\n",
       "      <td>[0.04678269475698471, 0.005143633112311363, 0....</td>\n",
       "      <td>0.474238</td>\n",
       "      <td>1.025438</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>R_5nfnwmO1TeGrdUr</td>\n",
       "      <td>True</td>\n",
       "      <td>New research: Psychiatric hospitalization help...</td>\n",
       "      <td>Hey, young changemakers! Did you know gun viol...</td>\n",
       "      <td>[new, research, psychiatric, hospitalization, ...</td>\n",
       "      <td>[hey, young, changemakers, did, you, know, gun...</td>\n",
       "      <td>0.641647</td>\n",
       "      <td>0.684093</td>\n",
       "      <td>[-0.006249735597521067, -0.03523017093539238, ...</td>\n",
       "      <td>[0.04514514282345772, 0.0019595555495470762, 0...</td>\n",
       "      <td>0.377887</td>\n",
       "      <td>1.115449</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>R_27qh4YB11K9SzrE</td>\n",
       "      <td>True</td>\n",
       "      <td>Unlocking the path to better suicide preventio...</td>\n",
       "      <td>Black individuals in the US face disproportion...</td>\n",
       "      <td>[unlocking, the, path, to, better, suicide, pr...</td>\n",
       "      <td>[black, individuals, in, the, us, face, dispro...</td>\n",
       "      <td>0.759442</td>\n",
       "      <td>0.721323</td>\n",
       "      <td>[-0.006094199605286121, -0.04629378020763397, ...</td>\n",
       "      <td>[0.03564606234431267, 0.016249382868409157, 0....</td>\n",
       "      <td>0.487959</td>\n",
       "      <td>1.011970</td>\n",
       "      <td>0.053201</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>R_8eUY7id8CizzXcI</td>\n",
       "      <td>True</td>\n",
       "      <td>New #research uncovers mixed findings on the i...</td>\n",
       "      <td>Study uncovers the complex relationship betwee...</td>\n",
       "      <td>[new, research, uncovers, mixed, findings, on,...</td>\n",
       "      <td>[study, uncovers, the, complex, relationship, ...</td>\n",
       "      <td>0.805752</td>\n",
       "      <td>0.707505</td>\n",
       "      <td>[-0.013588841073215008, -0.03418172895908356, ...</td>\n",
       "      <td>[0.0180619265884161, 0.005536293610930443, 0.0...</td>\n",
       "      <td>0.513994</td>\n",
       "      <td>0.985906</td>\n",
       "      <td>0.842869</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>R_2LYxh85b5wL9ENM</td>\n",
       "      <td>True</td>\n",
       "      <td>üîç New Study: Researchers examined psychiatric ...</td>\n",
       "      <td>\"üö® New study alert! 3015 Black adults reveal l...</td>\n",
       "      <td>[üîç, new, study, researchers, examined, psychia...</td>\n",
       "      <td>[üö®, new, study, alert, 3015, black, adults, re...</td>\n",
       "      <td>0.738415</td>\n",
       "      <td>0.710760</td>\n",
       "      <td>[-0.03319497033953667, -0.02609417587518692, 0...</td>\n",
       "      <td>[0.012063759379088879, 0.03258507326245308, 0....</td>\n",
       "      <td>0.518228</td>\n",
       "      <td>0.981602</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>R_5CG4CuJ7aufRQ0F</td>\n",
       "      <td>True</td>\n",
       "      <td>A new study sheds light on the impact of psych...</td>\n",
       "      <td>\"New study reveals alarming link between gun v...</td>\n",
       "      <td>[a, new, study, sheds, light, on, the, impact,...</td>\n",
       "      <td>[new, study, reveals, alarming, link, between,...</td>\n",
       "      <td>0.753497</td>\n",
       "      <td>0.678394</td>\n",
       "      <td>[-0.020886152982711792, -0.05574535205960274, ...</td>\n",
       "      <td>[0.022571489214897156, 0.005840015597641468, -...</td>\n",
       "      <td>0.493134</td>\n",
       "      <td>1.006843</td>\n",
       "      <td>0.580328</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>R_2vGZPubt3GyPsDn</td>\n",
       "      <td>True</td>\n",
       "      <td>Study: Tailored Psychiatric Care Reduces Suici...</td>\n",
       "      <td>New research indicates a link between gun viol...</td>\n",
       "      <td>[study, tailored, psychiatric, care, reduces, ...</td>\n",
       "      <td>[new, research, indicates, a, link, between, g...</td>\n",
       "      <td>0.758088</td>\n",
       "      <td>0.930462</td>\n",
       "      <td>[-0.004282189533114433, -0.009828958660364151,...</td>\n",
       "      <td>[0.013827767223119736, 0.02441759593784809, 0....</td>\n",
       "      <td>0.434926</td>\n",
       "      <td>1.063084</td>\n",
       "      <td>0.066172</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>R_7eku1c6ayrd15Jr</td>\n",
       "      <td>True</td>\n",
       "      <td>New research delves into the effects of psychi...</td>\n",
       "      <td>Recent study shows Black adults exposed to gun...</td>\n",
       "      <td>[new, research, delves, into, the, effects, of...</td>\n",
       "      <td>[recent, study, shows, black, adults, exposed,...</td>\n",
       "      <td>0.801087</td>\n",
       "      <td>0.831340</td>\n",
       "      <td>[-0.014171182177960873, -0.02652914449572563, ...</td>\n",
       "      <td>[0.04084155708551407, 0.01270886231213808, 0.0...</td>\n",
       "      <td>0.518258</td>\n",
       "      <td>0.981572</td>\n",
       "      <td>0.503083</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>R_2ebDNEgYTsOEQf0</td>\n",
       "      <td>True</td>\n",
       "      <td>Ever wondered about the effectiveness of psych...</td>\n",
       "      <td>New study: Black adults in the US exposed to g...</td>\n",
       "      <td>[ever, wondered, about, the, effectiveness, of...</td>\n",
       "      <td>[new, study, black, adults, in, the, us, expos...</td>\n",
       "      <td>0.732941</td>\n",
       "      <td>0.709669</td>\n",
       "      <td>[-0.010589778423309326, -0.030861640349030495,...</td>\n",
       "      <td>[0.032247330993413925, 0.027409091591835022, 0...</td>\n",
       "      <td>0.473391</td>\n",
       "      <td>1.026264</td>\n",
       "      <td>0.643730</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>R_4WrvkuJSjYeIg25</td>\n",
       "      <td>True</td>\n",
       "      <td>\"New study underscores the nuanced impact of p...</td>\n",
       "      <td>\"New study reveals alarming link between gun v...</td>\n",
       "      <td>[new, study, underscores, the, nuanced, impact...</td>\n",
       "      <td>[new, study, reveals, alarming, link, between,...</td>\n",
       "      <td>0.737928</td>\n",
       "      <td>0.739849</td>\n",
       "      <td>[-0.029109572991728783, -0.03090498223900795, ...</td>\n",
       "      <td>[0.016291270032525063, 0.01149387750774622, 0....</td>\n",
       "      <td>0.501323</td>\n",
       "      <td>0.998676</td>\n",
       "      <td>0.056150</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>R_2K9ZkIAyY4clGIC</td>\n",
       "      <td>True</td>\n",
       "      <td>\"üö® New study alert! üö® Did you know that hospit...</td>\n",
       "      <td>New study reveals alarming link between gun vi...</td>\n",
       "      <td>[üö®, new, study, alert, üö®, did, you, know, that...</td>\n",
       "      <td>[new, study, reveals, alarming, link, between,...</td>\n",
       "      <td>0.799230</td>\n",
       "      <td>0.699844</td>\n",
       "      <td>[-0.018360931426286697, -0.026469938457012177,...</td>\n",
       "      <td>[0.03405093774199486, 0.014812045730650425, 0....</td>\n",
       "      <td>0.496235</td>\n",
       "      <td>1.003758</td>\n",
       "      <td>0.748736</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>R_7TEOjOj8yFvQmG3</td>\n",
       "      <td>True</td>\n",
       "      <td>A recent study sheds light on the effectivenes...</td>\n",
       "      <td>Disturbing findings: US study reveals the conc...</td>\n",
       "      <td>[a, recent, study, sheds, light, on, the, effe...</td>\n",
       "      <td>[disturbing, findings, us, study, reveals, the...</td>\n",
       "      <td>0.766199</td>\n",
       "      <td>0.730464</td>\n",
       "      <td>[-0.008039155043661594, -0.029176948592066765,...</td>\n",
       "      <td>[0.026246629655361176, 0.0016223049024119973, ...</td>\n",
       "      <td>0.443774</td>\n",
       "      <td>1.054729</td>\n",
       "      <td>0.577964</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>R_58M6KLotmoyH2DT</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n\"Exciting findings on psychiatric hospitaliz...</td>\n",
       "      <td>New research alert! Published by the American...</td>\n",
       "      <td>[exciting, findings, on, psychiatric, hospital...</td>\n",
       "      <td>[new, research, alert, published, by, the, ame...</td>\n",
       "      <td>0.754753</td>\n",
       "      <td>0.666566</td>\n",
       "      <td>[-0.021623197942972183, -0.012901641428470612,...</td>\n",
       "      <td>[0.03077046014368534, 0.032802678644657135, 0....</td>\n",
       "      <td>0.500231</td>\n",
       "      <td>0.999769</td>\n",
       "      <td>0.049574</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>FS_7RNej3CNgjtmCzL</td>\n",
       "      <td>False</td>\n",
       "      <td>Retrospective analysis of 196,610 veterans wit...</td>\n",
       "      <td>Study based on survey data from 3015 Black US ...</td>\n",
       "      <td>[retrospective, analysis, of, 196610, veterans...</td>\n",
       "      <td>[study, based, on, survey, data, from, 3015, b...</td>\n",
       "      <td>0.737111</td>\n",
       "      <td>0.905836</td>\n",
       "      <td>[-0.01690417341887951, -0.015934668481349945, ...</td>\n",
       "      <td>[0.013109339401125908, 0.01700422540307045, 0....</td>\n",
       "      <td>0.474307</td>\n",
       "      <td>1.025371</td>\n",
       "      <td>0.042693</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>FS_2UXbdEM7RSuiIRD</td>\n",
       "      <td>False</td>\n",
       "      <td>üî¨New Study Alert! Research by @AmerMedicalAssn...</td>\n",
       "      <td>A recent study by @AmerMedicalAssn sheds light...</td>\n",
       "      <td>[üî¨new, study, alert, research, by, amermedical...</td>\n",
       "      <td>[a, recent, study, by, amermedicalassn, sheds,...</td>\n",
       "      <td>0.815317</td>\n",
       "      <td>0.657542</td>\n",
       "      <td>[-0.026290541514754295, -0.011911327950656414,...</td>\n",
       "      <td>[0.016674675047397614, 0.02445284090936184, 0....</td>\n",
       "      <td>0.528731</td>\n",
       "      <td>0.970844</td>\n",
       "      <td>0.047542</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>FS_8OJtGOjfFaucT5S</td>\n",
       "      <td>False</td>\n",
       "      <td>üîç New Research Alert! üß† Investigating the impa...</td>\n",
       "      <td>New study reveals alarming link between gun vi...</td>\n",
       "      <td>[üîç, new, research, alert, üß†, investigating, th...</td>\n",
       "      <td>[new, study, reveals, alarming, link, between,...</td>\n",
       "      <td>0.786073</td>\n",
       "      <td>0.765947</td>\n",
       "      <td>[-0.02686740830540657, -0.01266571506857872, 0...</td>\n",
       "      <td>[0.021941566839814186, 0.009445374831557274, 0...</td>\n",
       "      <td>0.470073</td>\n",
       "      <td>1.029492</td>\n",
       "      <td>0.123272</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    group          ResponseId  Finished  \\\n",
       "0       2   R_4dnZuujLN4Mn9um      True   \n",
       "1       1   R_6G8TRK7mezCmvlE      True   \n",
       "2       2   R_7GUlxEwsm4ANY9n      True   \n",
       "3       2   R_2fPKnwwJ3SXlNdN      True   \n",
       "4       1   R_2saDzTZSaCPpZNs      True   \n",
       "5       1   R_2g7nDxYeFPr352v      True   \n",
       "6       1   R_63y8AAf8TLPGRO3      True   \n",
       "7       2   R_2Pe7Wu3hNtg2ZU6      True   \n",
       "8       2   R_4Ld8n99yqQacpmu      True   \n",
       "9       1   R_7EnJjQXJawFTRwm      True   \n",
       "10      1   R_7uPaeBlCYnge9ax      True   \n",
       "11      1   R_2tnxsmidy0Q9YQ1      True   \n",
       "12      2   R_7sdyQIA4VE7WPTo      True   \n",
       "13      1   R_7IfLndj9JNO63Vw      True   \n",
       "14      1   R_5nfnwmO1TeGrdUr      True   \n",
       "15      1   R_27qh4YB11K9SzrE      True   \n",
       "16      1   R_8eUY7id8CizzXcI      True   \n",
       "17      1   R_2LYxh85b5wL9ENM      True   \n",
       "18      1   R_5CG4CuJ7aufRQ0F      True   \n",
       "19      1   R_2vGZPubt3GyPsDn      True   \n",
       "20      2   R_7eku1c6ayrd15Jr      True   \n",
       "21      2   R_2ebDNEgYTsOEQf0      True   \n",
       "22      2   R_4WrvkuJSjYeIg25      True   \n",
       "23      2   R_2K9ZkIAyY4clGIC      True   \n",
       "24      1   R_7TEOjOj8yFvQmG3      True   \n",
       "25      2   R_58M6KLotmoyH2DT      True   \n",
       "26      2  FS_7RNej3CNgjtmCzL     False   \n",
       "27      2  FS_2UXbdEM7RSuiIRD     False   \n",
       "28      1  FS_8OJtGOjfFaucT5S     False   \n",
       "\n",
       "                                            writing_a  \\\n",
       "0   \"New study reveals complex impact of psychiatr...   \n",
       "1   üß†üí° New Insights in Managing Suicide Risk! Rece...   \n",
       "2   \"New study finds psychiatric hospitalization r...   \n",
       "3   Psychiatric hospitalization reduces suicide ri...   \n",
       "4   \"New study in @JAMAPsych shows psychiatric hos...   \n",
       "5   Unraveling the puzzle of psychiatric hospitali...   \n",
       "6   New study reveals: Psychiatric hospitalization...   \n",
       "7   üîç New study finds psychiatric hospitalization ...   \n",
       "8   \"üí° Think psychiatric hospitalization's the ult...   \n",
       "9   New study shows psychiatric hospitalization re...   \n",
       "10  üåü Our latest research results provide smarter,...   \n",
       "11  A study examined psychiatric hospitalization's...   \n",
       "12  Hey there, curious minds! üåü Discover how 'brai...   \n",
       "13  üìä Unlock insights into psychiatric hospitaliza...   \n",
       "14  New research: Psychiatric hospitalization help...   \n",
       "15  Unlocking the path to better suicide preventio...   \n",
       "16  New #research uncovers mixed findings on the i...   \n",
       "17  üîç New Study: Researchers examined psychiatric ...   \n",
       "18  A new study sheds light on the impact of psych...   \n",
       "19  Study: Tailored Psychiatric Care Reduces Suici...   \n",
       "20  New research delves into the effects of psychi...   \n",
       "21  Ever wondered about the effectiveness of psych...   \n",
       "22  \"New study underscores the nuanced impact of p...   \n",
       "23  \"üö® New study alert! üö® Did you know that hospit...   \n",
       "24  A recent study sheds light on the effectivenes...   \n",
       "25  \\n\"Exciting findings on psychiatric hospitaliz...   \n",
       "26  Retrospective analysis of 196,610 veterans wit...   \n",
       "27  üî¨New Study Alert! Research by @AmerMedicalAssn...   \n",
       "28  üîç New Research Alert! üß† Investigating the impa...   \n",
       "\n",
       "                                            writing_b  \\\n",
       "0   \\n\"Study finds alarming link between gun viole...   \n",
       "1   üö® New study reveals a stark reality: Exposure ...   \n",
       "2   A Study on 3015 Black adults shows GVE signifi...   \n",
       "3   New study by @AMAJournal reveals alarming asso...   \n",
       "4   \"New research suggests that reducing gun viole...   \n",
       "5   \"Research shows stark connections: Gun violenc...   \n",
       "6   \"New study reveals a strong link between gun v...   \n",
       "7   üî´üß† Study finds Black adults exposed to gun vio...   \n",
       "8   \"New study finds alarming link between gun vio...   \n",
       "9   \"Study reveals Black individuals exposed to gu...   \n",
       "10  \"üí° New study reveals a strong link between gun...   \n",
       "11  \\nNew study reveals alarming link between gun ...   \n",
       "12  Hey politicians, wake up! Black Americans face...   \n",
       "13  üíî Heartbreaking findings: Black individuals fa...   \n",
       "14  Hey, young changemakers! Did you know gun viol...   \n",
       "15  Black individuals in the US face disproportion...   \n",
       "16  Study uncovers the complex relationship betwee...   \n",
       "17  \"üö® New study alert! 3015 Black adults reveal l...   \n",
       "18  \"New study reveals alarming link between gun v...   \n",
       "19  New research indicates a link between gun viol...   \n",
       "20  Recent study shows Black adults exposed to gun...   \n",
       "21  New study: Black adults in the US exposed to g...   \n",
       "22  \"New study reveals alarming link between gun v...   \n",
       "23  New study reveals alarming link between gun vi...   \n",
       "24  Disturbing findings: US study reveals the conc...   \n",
       "25   New research alert! Published by the American...   \n",
       "26  Study based on survey data from 3015 Black US ...   \n",
       "27  A recent study by @AmerMedicalAssn sheds light...   \n",
       "28  New study reveals alarming link between gun vi...   \n",
       "\n",
       "                                  writing_a_clean_lst  \\\n",
       "0   [new, study, reveals, complex, impact, of, psy...   \n",
       "1   [üß†üí°, new, insights, in, managing, suicide, ris...   \n",
       "2   [new, study, finds, psychiatric, hospitalizati...   \n",
       "3   [psychiatric, hospitalization, reduces, suicid...   \n",
       "4   [new, study, in, jamapsych, shows, psychiatric...   \n",
       "5   [unraveling, the, puzzle, of, psychiatric, hos...   \n",
       "6   [new, study, reveals, psychiatric, hospitaliza...   \n",
       "7   [üîç, new, study, finds, psychiatric, hospitaliz...   \n",
       "8   [üí°, think, psychiatric, hospitalizations, the,...   \n",
       "9   [new, study, shows, psychiatric, hospitalizati...   \n",
       "10  [üåü, our, latest, research, results, provide, s...   \n",
       "11  [a, study, examined, psychiatric, hospitalizat...   \n",
       "12  [hey, there, curious, minds, üåü, discover, how,...   \n",
       "13  [üìä, unlock, insights, into, psychiatric, hospi...   \n",
       "14  [new, research, psychiatric, hospitalization, ...   \n",
       "15  [unlocking, the, path, to, better, suicide, pr...   \n",
       "16  [new, research, uncovers, mixed, findings, on,...   \n",
       "17  [üîç, new, study, researchers, examined, psychia...   \n",
       "18  [a, new, study, sheds, light, on, the, impact,...   \n",
       "19  [study, tailored, psychiatric, care, reduces, ...   \n",
       "20  [new, research, delves, into, the, effects, of...   \n",
       "21  [ever, wondered, about, the, effectiveness, of...   \n",
       "22  [new, study, underscores, the, nuanced, impact...   \n",
       "23  [üö®, new, study, alert, üö®, did, you, know, that...   \n",
       "24  [a, recent, study, sheds, light, on, the, effe...   \n",
       "25  [exciting, findings, on, psychiatric, hospital...   \n",
       "26  [retrospective, analysis, of, 196610, veterans...   \n",
       "27  [üî¨new, study, alert, research, by, amermedical...   \n",
       "28  [üîç, new, research, alert, üß†, investigating, th...   \n",
       "\n",
       "                                  writing_b_clean_lst  cosine_similarity_w2v  \\\n",
       "0   [study, finds, alarming, link, between, gun, v...               0.810496   \n",
       "1   [üö®, new, study, reveals, a, stark, reality, ex...               0.825563   \n",
       "2   [a, study, on, 3015, black, adults, shows, gve...               0.740004   \n",
       "3   [new, study, by, amajournal, reveals, alarming...               0.783746   \n",
       "4   [new, research, suggests, that, reducing, gun,...               0.682288   \n",
       "5   [research, shows, stark, connections, gun, vio...               0.809810   \n",
       "6   [new, study, reveals, a, strong, link, between...               0.772005   \n",
       "7   [üî´üß†, study, finds, black, adults, exposed, to,...               0.731112   \n",
       "8   [new, study, finds, alarming, link, between, g...               0.809953   \n",
       "9   [study, reveals, black, individuals, exposed, ...               0.772831   \n",
       "10  [üí°, new, study, reveals, a, strong, link, betw...               0.727985   \n",
       "11  [new, study, reveals, alarming, link, between,...               0.741933   \n",
       "12  [hey, politicians, wake, up, black, americans,...               0.695317   \n",
       "13  [üíî, heartbreaking, findings, black, individual...               0.673836   \n",
       "14  [hey, young, changemakers, did, you, know, gun...               0.641647   \n",
       "15  [black, individuals, in, the, us, face, dispro...               0.759442   \n",
       "16  [study, uncovers, the, complex, relationship, ...               0.805752   \n",
       "17  [üö®, new, study, alert, 3015, black, adults, re...               0.738415   \n",
       "18  [new, study, reveals, alarming, link, between,...               0.753497   \n",
       "19  [new, research, indicates, a, link, between, g...               0.758088   \n",
       "20  [recent, study, shows, black, adults, exposed,...               0.801087   \n",
       "21  [new, study, black, adults, in, the, us, expos...               0.732941   \n",
       "22  [new, study, reveals, alarming, link, between,...               0.737928   \n",
       "23  [new, study, reveals, alarming, link, between,...               0.799230   \n",
       "24  [disturbing, findings, us, study, reveals, the...               0.766199   \n",
       "25  [new, research, alert, published, by, the, ame...               0.754753   \n",
       "26  [study, based, on, survey, data, from, 3015, b...               0.737111   \n",
       "27  [a, recent, study, by, amermedicalassn, sheds,...               0.815317   \n",
       "28  [new, study, reveals, alarming, link, between,...               0.786073   \n",
       "\n",
       "    unnormalized_dot_product_w2v  \\\n",
       "0                       0.713841   \n",
       "1                       0.754296   \n",
       "2                       0.790497   \n",
       "3                       0.683887   \n",
       "4                       0.733727   \n",
       "5                       0.757519   \n",
       "6                       0.815978   \n",
       "7                       0.827849   \n",
       "8                       0.653381   \n",
       "9                       0.963211   \n",
       "10                      0.809747   \n",
       "11                      0.731070   \n",
       "12                      0.639158   \n",
       "13                      0.750695   \n",
       "14                      0.684093   \n",
       "15                      0.721323   \n",
       "16                      0.707505   \n",
       "17                      0.710760   \n",
       "18                      0.678394   \n",
       "19                      0.930462   \n",
       "20                      0.831340   \n",
       "21                      0.709669   \n",
       "22                      0.739849   \n",
       "23                      0.699844   \n",
       "24                      0.730464   \n",
       "25                      0.666566   \n",
       "26                      0.905836   \n",
       "27                      0.657542   \n",
       "28                      0.765947   \n",
       "\n",
       "                                   openai_embedding_a  \\\n",
       "0   [-0.025056514889001846, -0.03770783916115761, ...   \n",
       "1   [-0.018861589953303337, -0.020371491089463234,...   \n",
       "2   [-0.015813207253813744, -0.028458785265684128,...   \n",
       "3   [-0.006001750472933054, -0.028704537078738213,...   \n",
       "4   [-0.02892470918595791, -0.03582962229847908, 0...   \n",
       "5   [-0.0008465779246762395, -0.024789869785308838...   \n",
       "6   [-0.025526609271764755, -0.01785901188850403, ...   \n",
       "7   [-0.042180195450782776, -0.018665548413991928,...   \n",
       "8   [-0.011265125125646591, -0.04513534903526306, ...   \n",
       "9   [-0.01255673449486494, -0.03516360744833946, 0...   \n",
       "10  [0.027828657999634743, -0.01811208575963974, 0...   \n",
       "11  [-0.01983460783958435, -0.05631333217024803, 0...   \n",
       "12  [0.005720171611756086, -0.020700884982943535, ...   \n",
       "13  [-0.015350842848420143, -0.04711201786994934, ...   \n",
       "14  [-0.006249735597521067, -0.03523017093539238, ...   \n",
       "15  [-0.006094199605286121, -0.04629378020763397, ...   \n",
       "16  [-0.013588841073215008, -0.03418172895908356, ...   \n",
       "17  [-0.03319497033953667, -0.02609417587518692, 0...   \n",
       "18  [-0.020886152982711792, -0.05574535205960274, ...   \n",
       "19  [-0.004282189533114433, -0.009828958660364151,...   \n",
       "20  [-0.014171182177960873, -0.02652914449572563, ...   \n",
       "21  [-0.010589778423309326, -0.030861640349030495,...   \n",
       "22  [-0.029109572991728783, -0.03090498223900795, ...   \n",
       "23  [-0.018360931426286697, -0.026469938457012177,...   \n",
       "24  [-0.008039155043661594, -0.029176948592066765,...   \n",
       "25  [-0.021623197942972183, -0.012901641428470612,...   \n",
       "26  [-0.01690417341887951, -0.015934668481349945, ...   \n",
       "27  [-0.026290541514754295, -0.011911327950656414,...   \n",
       "28  [-0.02686740830540657, -0.01266571506857872, 0...   \n",
       "\n",
       "                                   openai_embedding_b  \\\n",
       "0   [0.023708993569016457, 0.02937544323503971, 0....   \n",
       "1   [0.020731830969452858, 0.016379600390791893, 0...   \n",
       "2   [0.04760603606700897, 0.021079791709780693, 0....   \n",
       "3   [0.025258364155888557, 0.012829821556806564, 0...   \n",
       "4   [0.054963596165180206, 0.011303708888590336, 0...   \n",
       "5   [0.022167474031448364, 0.019238609820604324, 0...   \n",
       "6   [0.021031426265835762, 0.018929390236735344, 0...   \n",
       "7   [0.03014543652534485, 0.04210395738482475, 0.0...   \n",
       "8   [0.0172509104013443, 0.037992093712091446, 0.0...   \n",
       "9   [0.02446923218667507, 0.013484450988471508, 0....   \n",
       "10  [0.0053543285466730595, 0.011121646501123905, ...   \n",
       "11  [0.01820780709385872, 0.02473226934671402, -4....   \n",
       "12  [0.05163086950778961, 0.02711629867553711, 0.0...   \n",
       "13  [0.04678269475698471, 0.005143633112311363, 0....   \n",
       "14  [0.04514514282345772, 0.0019595555495470762, 0...   \n",
       "15  [0.03564606234431267, 0.016249382868409157, 0....   \n",
       "16  [0.0180619265884161, 0.005536293610930443, 0.0...   \n",
       "17  [0.012063759379088879, 0.03258507326245308, 0....   \n",
       "18  [0.022571489214897156, 0.005840015597641468, -...   \n",
       "19  [0.013827767223119736, 0.02441759593784809, 0....   \n",
       "20  [0.04084155708551407, 0.01270886231213808, 0.0...   \n",
       "21  [0.032247330993413925, 0.027409091591835022, 0...   \n",
       "22  [0.016291270032525063, 0.01149387750774622, 0....   \n",
       "23  [0.03405093774199486, 0.014812045730650425, 0....   \n",
       "24  [0.026246629655361176, 0.0016223049024119973, ...   \n",
       "25  [0.03077046014368534, 0.032802678644657135, 0....   \n",
       "26  [0.013109339401125908, 0.01700422540307045, 0....   \n",
       "27  [0.016674675047397614, 0.02445284090936184, 0....   \n",
       "28  [0.021941566839814186, 0.009445374831557274, 0...   \n",
       "\n",
       "    cosine_similarity_openai  euclidean_distance_openai  \\\n",
       "0                   0.448127                   1.050593   \n",
       "1                   0.549861                   0.948830   \n",
       "2                   0.412520                   1.083956   \n",
       "3                   0.507656                   0.992314   \n",
       "4                   0.522809                   0.976924   \n",
       "5                   0.516120                   0.983748   \n",
       "6                   0.551211                   0.947406   \n",
       "7                   0.481462                   1.018370   \n",
       "8                   0.442072                   1.056341   \n",
       "9                   0.570545                   0.926774   \n",
       "10                  0.392544                   1.102231   \n",
       "11                  0.385038                   1.109019   \n",
       "12                  0.313337                   1.171890   \n",
       "13                  0.474238                   1.025438   \n",
       "14                  0.377887                   1.115449   \n",
       "15                  0.487959                   1.011970   \n",
       "16                  0.513994                   0.985906   \n",
       "17                  0.518228                   0.981602   \n",
       "18                  0.493134                   1.006843   \n",
       "19                  0.434926                   1.063084   \n",
       "20                  0.518258                   0.981572   \n",
       "21                  0.473391                   1.026264   \n",
       "22                  0.501323                   0.998676   \n",
       "23                  0.496235                   1.003758   \n",
       "24                  0.443774                   1.054729   \n",
       "25                  0.500231                   0.999769   \n",
       "26                  0.474307                   1.025371   \n",
       "27                  0.528731                   0.970844   \n",
       "28                  0.470073                   1.029492   \n",
       "\n",
       "    lda_cosine_similarity  journalist  \n",
       "0                0.524188           1  \n",
       "1                0.038555           2  \n",
       "2                0.043443           3  \n",
       "3                0.047468           4  \n",
       "4                0.750716           5  \n",
       "5                0.249582           6  \n",
       "6                0.047959           7  \n",
       "7                0.998329           8  \n",
       "8                0.041147           9  \n",
       "9                0.999906          10  \n",
       "10               0.069651          11  \n",
       "11               0.954322          12  \n",
       "12               0.999935          13  \n",
       "13               0.999999          14  \n",
       "14               0.999979          15  \n",
       "15               0.053201          16  \n",
       "16               0.842869          17  \n",
       "17               0.999958          18  \n",
       "18               0.580328          19  \n",
       "19               0.066172          20  \n",
       "20               0.503083          21  \n",
       "21               0.643730          22  \n",
       "22               0.056150          23  \n",
       "23               0.748736          24  \n",
       "24               0.577964          25  \n",
       "25               0.049574          26  \n",
       "26               0.042693          27  \n",
       "27               0.047542          28  \n",
       "28               0.123272          29  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the similarity score matrix\n",
    "# Create similarity matrices\n",
    "num_users = len(merged_df) # 29\n",
    "similarity_matrix_a = np.zeros((num_users, num_users))\n",
    "similarity_matrix_b = np.zeros((num_users, num_users))\n",
    "\n",
    "# Calculate the similarity for writing_a\n",
    "for i in range(num_users):\n",
    "    for j in range(num_users):\n",
    "        similarity_matrix_a[i, j] = get_cosine_similarity(merged_df.loc[i, 'writing_a_clean_lst'], merged_df.loc[j, 'writing_a_clean_lst'])\n",
    "        similarity_matrix_b[i, j] = get_cosine_similarity(merged_df.loc[i, 'writing_b_clean_lst'], merged_df.loc[j, 'writing_b_clean_lst'])\n",
    "\n",
    "# Convert matrices to DataFrames for better readability\n",
    "similarity_df_a = pd.DataFrame(similarity_matrix_a, columns=list(range(1,30)))\n",
    "similarity_df_b = pd.DataFrame(similarity_matrix_b, columns=list(range(1,30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df_a.to_excel('../Data/between_subject_similarity_matrix_a.xlsx', index=False)\n",
    "similarity_df_b.to_excel('../Data/between_subject_similarity_matrix_b.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.897777</td>\n",
       "      <td>0.865769</td>\n",
       "      <td>0.927274</td>\n",
       "      <td>0.814605</td>\n",
       "      <td>0.849445</td>\n",
       "      <td>0.897746</td>\n",
       "      <td>0.899409</td>\n",
       "      <td>0.888698</td>\n",
       "      <td>0.843190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.885971</td>\n",
       "      <td>0.857606</td>\n",
       "      <td>0.893579</td>\n",
       "      <td>0.873972</td>\n",
       "      <td>0.980201</td>\n",
       "      <td>0.883580</td>\n",
       "      <td>0.829955</td>\n",
       "      <td>0.889992</td>\n",
       "      <td>0.852783</td>\n",
       "      <td>0.851562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.897777</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886390</td>\n",
       "      <td>0.882154</td>\n",
       "      <td>0.864359</td>\n",
       "      <td>0.858079</td>\n",
       "      <td>0.926472</td>\n",
       "      <td>0.893035</td>\n",
       "      <td>0.868674</td>\n",
       "      <td>0.866818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.933510</td>\n",
       "      <td>0.861892</td>\n",
       "      <td>0.910783</td>\n",
       "      <td>0.848377</td>\n",
       "      <td>0.902980</td>\n",
       "      <td>0.877640</td>\n",
       "      <td>0.813705</td>\n",
       "      <td>0.894445</td>\n",
       "      <td>0.823056</td>\n",
       "      <td>0.850992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.865769</td>\n",
       "      <td>0.886390</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896055</td>\n",
       "      <td>0.839453</td>\n",
       "      <td>0.868302</td>\n",
       "      <td>0.891039</td>\n",
       "      <td>0.851946</td>\n",
       "      <td>0.888574</td>\n",
       "      <td>0.828325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862271</td>\n",
       "      <td>0.894841</td>\n",
       "      <td>0.904236</td>\n",
       "      <td>0.840616</td>\n",
       "      <td>0.878080</td>\n",
       "      <td>0.859450</td>\n",
       "      <td>0.898116</td>\n",
       "      <td>0.866050</td>\n",
       "      <td>0.841487</td>\n",
       "      <td>0.893639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.927274</td>\n",
       "      <td>0.882154</td>\n",
       "      <td>0.896055</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.814755</td>\n",
       "      <td>0.824425</td>\n",
       "      <td>0.892607</td>\n",
       "      <td>0.869157</td>\n",
       "      <td>0.885526</td>\n",
       "      <td>0.836706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872986</td>\n",
       "      <td>0.856752</td>\n",
       "      <td>0.893440</td>\n",
       "      <td>0.848497</td>\n",
       "      <td>0.942968</td>\n",
       "      <td>0.874131</td>\n",
       "      <td>0.866142</td>\n",
       "      <td>0.870952</td>\n",
       "      <td>0.836712</td>\n",
       "      <td>0.883679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.814605</td>\n",
       "      <td>0.864359</td>\n",
       "      <td>0.839453</td>\n",
       "      <td>0.814755</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.788036</td>\n",
       "      <td>0.867569</td>\n",
       "      <td>0.789142</td>\n",
       "      <td>0.840695</td>\n",
       "      <td>0.767226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.876847</td>\n",
       "      <td>0.814360</td>\n",
       "      <td>0.839335</td>\n",
       "      <td>0.789835</td>\n",
       "      <td>0.831192</td>\n",
       "      <td>0.813671</td>\n",
       "      <td>0.846855</td>\n",
       "      <td>0.769716</td>\n",
       "      <td>0.802049</td>\n",
       "      <td>0.817092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.849445</td>\n",
       "      <td>0.858079</td>\n",
       "      <td>0.868302</td>\n",
       "      <td>0.824425</td>\n",
       "      <td>0.788036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.855494</td>\n",
       "      <td>0.845567</td>\n",
       "      <td>0.894111</td>\n",
       "      <td>0.861864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869659</td>\n",
       "      <td>0.887042</td>\n",
       "      <td>0.858907</td>\n",
       "      <td>0.880448</td>\n",
       "      <td>0.869259</td>\n",
       "      <td>0.851684</td>\n",
       "      <td>0.855571</td>\n",
       "      <td>0.879893</td>\n",
       "      <td>0.838531</td>\n",
       "      <td>0.894163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.897746</td>\n",
       "      <td>0.926472</td>\n",
       "      <td>0.891039</td>\n",
       "      <td>0.892607</td>\n",
       "      <td>0.867569</td>\n",
       "      <td>0.855494</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.870008</td>\n",
       "      <td>0.913901</td>\n",
       "      <td>0.851123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931600</td>\n",
       "      <td>0.882980</td>\n",
       "      <td>0.895631</td>\n",
       "      <td>0.875633</td>\n",
       "      <td>0.916188</td>\n",
       "      <td>0.878453</td>\n",
       "      <td>0.860260</td>\n",
       "      <td>0.885978</td>\n",
       "      <td>0.849385</td>\n",
       "      <td>0.865981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.899409</td>\n",
       "      <td>0.893035</td>\n",
       "      <td>0.851946</td>\n",
       "      <td>0.869157</td>\n",
       "      <td>0.789142</td>\n",
       "      <td>0.845567</td>\n",
       "      <td>0.870008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.852274</td>\n",
       "      <td>0.880922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.874532</td>\n",
       "      <td>0.896833</td>\n",
       "      <td>0.946425</td>\n",
       "      <td>0.808719</td>\n",
       "      <td>0.899147</td>\n",
       "      <td>0.838027</td>\n",
       "      <td>0.786266</td>\n",
       "      <td>0.880878</td>\n",
       "      <td>0.755726</td>\n",
       "      <td>0.832224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.888698</td>\n",
       "      <td>0.868674</td>\n",
       "      <td>0.888574</td>\n",
       "      <td>0.885526</td>\n",
       "      <td>0.840695</td>\n",
       "      <td>0.894111</td>\n",
       "      <td>0.913901</td>\n",
       "      <td>0.852274</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.849303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.892467</td>\n",
       "      <td>0.884985</td>\n",
       "      <td>0.873981</td>\n",
       "      <td>0.911276</td>\n",
       "      <td>0.909959</td>\n",
       "      <td>0.889372</td>\n",
       "      <td>0.905538</td>\n",
       "      <td>0.867178</td>\n",
       "      <td>0.893887</td>\n",
       "      <td>0.908533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.843190</td>\n",
       "      <td>0.866818</td>\n",
       "      <td>0.828325</td>\n",
       "      <td>0.836706</td>\n",
       "      <td>0.767226</td>\n",
       "      <td>0.861864</td>\n",
       "      <td>0.851123</td>\n",
       "      <td>0.880922</td>\n",
       "      <td>0.849303</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872618</td>\n",
       "      <td>0.839869</td>\n",
       "      <td>0.849620</td>\n",
       "      <td>0.791084</td>\n",
       "      <td>0.866520</td>\n",
       "      <td>0.794668</td>\n",
       "      <td>0.767258</td>\n",
       "      <td>0.861310</td>\n",
       "      <td>0.751409</td>\n",
       "      <td>0.840067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.845380</td>\n",
       "      <td>0.840210</td>\n",
       "      <td>0.881216</td>\n",
       "      <td>0.826806</td>\n",
       "      <td>0.812519</td>\n",
       "      <td>0.912169</td>\n",
       "      <td>0.880090</td>\n",
       "      <td>0.854255</td>\n",
       "      <td>0.901191</td>\n",
       "      <td>0.832143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870869</td>\n",
       "      <td>0.877538</td>\n",
       "      <td>0.869662</td>\n",
       "      <td>0.888018</td>\n",
       "      <td>0.869199</td>\n",
       "      <td>0.853123</td>\n",
       "      <td>0.893081</td>\n",
       "      <td>0.865068</td>\n",
       "      <td>0.818603</td>\n",
       "      <td>0.893788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.907540</td>\n",
       "      <td>0.884934</td>\n",
       "      <td>0.875668</td>\n",
       "      <td>0.946640</td>\n",
       "      <td>0.838157</td>\n",
       "      <td>0.842756</td>\n",
       "      <td>0.895554</td>\n",
       "      <td>0.856726</td>\n",
       "      <td>0.912837</td>\n",
       "      <td>0.848631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.907210</td>\n",
       "      <td>0.862158</td>\n",
       "      <td>0.886034</td>\n",
       "      <td>0.867450</td>\n",
       "      <td>0.934163</td>\n",
       "      <td>0.874962</td>\n",
       "      <td>0.875416</td>\n",
       "      <td>0.881039</td>\n",
       "      <td>0.841396</td>\n",
       "      <td>0.916184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.797256</td>\n",
       "      <td>0.757732</td>\n",
       "      <td>0.772453</td>\n",
       "      <td>0.841225</td>\n",
       "      <td>0.800958</td>\n",
       "      <td>0.710923</td>\n",
       "      <td>0.796526</td>\n",
       "      <td>0.753601</td>\n",
       "      <td>0.797316</td>\n",
       "      <td>0.680390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766723</td>\n",
       "      <td>0.782304</td>\n",
       "      <td>0.798825</td>\n",
       "      <td>0.746512</td>\n",
       "      <td>0.812341</td>\n",
       "      <td>0.740174</td>\n",
       "      <td>0.788530</td>\n",
       "      <td>0.737813</td>\n",
       "      <td>0.763949</td>\n",
       "      <td>0.785362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.815203</td>\n",
       "      <td>0.845144</td>\n",
       "      <td>0.817059</td>\n",
       "      <td>0.818404</td>\n",
       "      <td>0.828072</td>\n",
       "      <td>0.841017</td>\n",
       "      <td>0.834774</td>\n",
       "      <td>0.827190</td>\n",
       "      <td>0.816113</td>\n",
       "      <td>0.817153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859192</td>\n",
       "      <td>0.833213</td>\n",
       "      <td>0.836468</td>\n",
       "      <td>0.768343</td>\n",
       "      <td>0.833017</td>\n",
       "      <td>0.783124</td>\n",
       "      <td>0.811284</td>\n",
       "      <td>0.818609</td>\n",
       "      <td>0.747984</td>\n",
       "      <td>0.828682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.783988</td>\n",
       "      <td>0.780024</td>\n",
       "      <td>0.753832</td>\n",
       "      <td>0.735353</td>\n",
       "      <td>0.789667</td>\n",
       "      <td>0.736287</td>\n",
       "      <td>0.781332</td>\n",
       "      <td>0.750503</td>\n",
       "      <td>0.747724</td>\n",
       "      <td>0.666582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749754</td>\n",
       "      <td>0.733695</td>\n",
       "      <td>0.781053</td>\n",
       "      <td>0.722573</td>\n",
       "      <td>0.777644</td>\n",
       "      <td>0.693600</td>\n",
       "      <td>0.746741</td>\n",
       "      <td>0.721366</td>\n",
       "      <td>0.724773</td>\n",
       "      <td>0.686771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.886961</td>\n",
       "      <td>0.860154</td>\n",
       "      <td>0.856344</td>\n",
       "      <td>0.864299</td>\n",
       "      <td>0.795156</td>\n",
       "      <td>0.846405</td>\n",
       "      <td>0.880638</td>\n",
       "      <td>0.840610</td>\n",
       "      <td>0.879507</td>\n",
       "      <td>0.811685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857478</td>\n",
       "      <td>0.870504</td>\n",
       "      <td>0.878393</td>\n",
       "      <td>0.918057</td>\n",
       "      <td>0.892985</td>\n",
       "      <td>0.878297</td>\n",
       "      <td>0.850727</td>\n",
       "      <td>0.862710</td>\n",
       "      <td>0.882088</td>\n",
       "      <td>0.864454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.726719</td>\n",
       "      <td>0.711603</td>\n",
       "      <td>0.758791</td>\n",
       "      <td>0.714789</td>\n",
       "      <td>0.720214</td>\n",
       "      <td>0.822133</td>\n",
       "      <td>0.728669</td>\n",
       "      <td>0.685988</td>\n",
       "      <td>0.813223</td>\n",
       "      <td>0.702363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.733064</td>\n",
       "      <td>0.764451</td>\n",
       "      <td>0.711773</td>\n",
       "      <td>0.815616</td>\n",
       "      <td>0.743748</td>\n",
       "      <td>0.793567</td>\n",
       "      <td>0.812558</td>\n",
       "      <td>0.727605</td>\n",
       "      <td>0.797270</td>\n",
       "      <td>0.834660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.822338</td>\n",
       "      <td>0.817655</td>\n",
       "      <td>0.838768</td>\n",
       "      <td>0.830438</td>\n",
       "      <td>0.818860</td>\n",
       "      <td>0.822453</td>\n",
       "      <td>0.849755</td>\n",
       "      <td>0.793364</td>\n",
       "      <td>0.862826</td>\n",
       "      <td>0.796336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.847940</td>\n",
       "      <td>0.802369</td>\n",
       "      <td>0.825285</td>\n",
       "      <td>0.831318</td>\n",
       "      <td>0.845693</td>\n",
       "      <td>0.843497</td>\n",
       "      <td>0.901613</td>\n",
       "      <td>0.803563</td>\n",
       "      <td>0.784669</td>\n",
       "      <td>0.850865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.909160</td>\n",
       "      <td>0.882821</td>\n",
       "      <td>0.872007</td>\n",
       "      <td>0.893242</td>\n",
       "      <td>0.835657</td>\n",
       "      <td>0.834136</td>\n",
       "      <td>0.890722</td>\n",
       "      <td>0.801153</td>\n",
       "      <td>0.913530</td>\n",
       "      <td>0.812816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.908361</td>\n",
       "      <td>0.833665</td>\n",
       "      <td>0.849931</td>\n",
       "      <td>0.882858</td>\n",
       "      <td>0.921141</td>\n",
       "      <td>0.890950</td>\n",
       "      <td>0.882847</td>\n",
       "      <td>0.874082</td>\n",
       "      <td>0.862578</td>\n",
       "      <td>0.873536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.885971</td>\n",
       "      <td>0.933510</td>\n",
       "      <td>0.862271</td>\n",
       "      <td>0.872986</td>\n",
       "      <td>0.876847</td>\n",
       "      <td>0.869659</td>\n",
       "      <td>0.931600</td>\n",
       "      <td>0.874532</td>\n",
       "      <td>0.892467</td>\n",
       "      <td>0.872618</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.870740</td>\n",
       "      <td>0.892927</td>\n",
       "      <td>0.866311</td>\n",
       "      <td>0.903511</td>\n",
       "      <td>0.873769</td>\n",
       "      <td>0.852776</td>\n",
       "      <td>0.901577</td>\n",
       "      <td>0.807337</td>\n",
       "      <td>0.878662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.857606</td>\n",
       "      <td>0.861892</td>\n",
       "      <td>0.894841</td>\n",
       "      <td>0.856752</td>\n",
       "      <td>0.814360</td>\n",
       "      <td>0.887042</td>\n",
       "      <td>0.882980</td>\n",
       "      <td>0.896833</td>\n",
       "      <td>0.884985</td>\n",
       "      <td>0.839869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870740</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921079</td>\n",
       "      <td>0.848599</td>\n",
       "      <td>0.864922</td>\n",
       "      <td>0.863425</td>\n",
       "      <td>0.856764</td>\n",
       "      <td>0.858994</td>\n",
       "      <td>0.817670</td>\n",
       "      <td>0.879877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.893579</td>\n",
       "      <td>0.910783</td>\n",
       "      <td>0.904236</td>\n",
       "      <td>0.893440</td>\n",
       "      <td>0.839335</td>\n",
       "      <td>0.858907</td>\n",
       "      <td>0.895631</td>\n",
       "      <td>0.946425</td>\n",
       "      <td>0.873981</td>\n",
       "      <td>0.849620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.892927</td>\n",
       "      <td>0.921079</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.847019</td>\n",
       "      <td>0.899236</td>\n",
       "      <td>0.867141</td>\n",
       "      <td>0.850989</td>\n",
       "      <td>0.877794</td>\n",
       "      <td>0.801870</td>\n",
       "      <td>0.872153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.873972</td>\n",
       "      <td>0.848377</td>\n",
       "      <td>0.840616</td>\n",
       "      <td>0.848497</td>\n",
       "      <td>0.789835</td>\n",
       "      <td>0.880448</td>\n",
       "      <td>0.875633</td>\n",
       "      <td>0.808719</td>\n",
       "      <td>0.911276</td>\n",
       "      <td>0.791084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866311</td>\n",
       "      <td>0.848599</td>\n",
       "      <td>0.847019</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.889096</td>\n",
       "      <td>0.902166</td>\n",
       "      <td>0.870294</td>\n",
       "      <td>0.850381</td>\n",
       "      <td>0.889137</td>\n",
       "      <td>0.906954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.980201</td>\n",
       "      <td>0.902980</td>\n",
       "      <td>0.878080</td>\n",
       "      <td>0.942968</td>\n",
       "      <td>0.831192</td>\n",
       "      <td>0.869259</td>\n",
       "      <td>0.916188</td>\n",
       "      <td>0.899147</td>\n",
       "      <td>0.909959</td>\n",
       "      <td>0.866520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903511</td>\n",
       "      <td>0.864922</td>\n",
       "      <td>0.899236</td>\n",
       "      <td>0.889096</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893366</td>\n",
       "      <td>0.855512</td>\n",
       "      <td>0.894610</td>\n",
       "      <td>0.860441</td>\n",
       "      <td>0.883367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.883580</td>\n",
       "      <td>0.877640</td>\n",
       "      <td>0.859450</td>\n",
       "      <td>0.874131</td>\n",
       "      <td>0.813671</td>\n",
       "      <td>0.851684</td>\n",
       "      <td>0.878453</td>\n",
       "      <td>0.838027</td>\n",
       "      <td>0.889372</td>\n",
       "      <td>0.794668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.873769</td>\n",
       "      <td>0.863425</td>\n",
       "      <td>0.867141</td>\n",
       "      <td>0.902166</td>\n",
       "      <td>0.893366</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.858422</td>\n",
       "      <td>0.855367</td>\n",
       "      <td>0.851260</td>\n",
       "      <td>0.892846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.829955</td>\n",
       "      <td>0.813705</td>\n",
       "      <td>0.898116</td>\n",
       "      <td>0.866142</td>\n",
       "      <td>0.846855</td>\n",
       "      <td>0.855571</td>\n",
       "      <td>0.860260</td>\n",
       "      <td>0.786266</td>\n",
       "      <td>0.905538</td>\n",
       "      <td>0.767258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852776</td>\n",
       "      <td>0.856764</td>\n",
       "      <td>0.850989</td>\n",
       "      <td>0.870294</td>\n",
       "      <td>0.855512</td>\n",
       "      <td>0.858422</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.824256</td>\n",
       "      <td>0.839475</td>\n",
       "      <td>0.901664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.889992</td>\n",
       "      <td>0.894445</td>\n",
       "      <td>0.866050</td>\n",
       "      <td>0.870952</td>\n",
       "      <td>0.769716</td>\n",
       "      <td>0.879893</td>\n",
       "      <td>0.885978</td>\n",
       "      <td>0.880878</td>\n",
       "      <td>0.867178</td>\n",
       "      <td>0.861310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901577</td>\n",
       "      <td>0.858994</td>\n",
       "      <td>0.877794</td>\n",
       "      <td>0.850381</td>\n",
       "      <td>0.894610</td>\n",
       "      <td>0.855367</td>\n",
       "      <td>0.824256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.802415</td>\n",
       "      <td>0.849126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.852783</td>\n",
       "      <td>0.823056</td>\n",
       "      <td>0.841487</td>\n",
       "      <td>0.836712</td>\n",
       "      <td>0.802049</td>\n",
       "      <td>0.838531</td>\n",
       "      <td>0.849385</td>\n",
       "      <td>0.755726</td>\n",
       "      <td>0.893887</td>\n",
       "      <td>0.751409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807337</td>\n",
       "      <td>0.817670</td>\n",
       "      <td>0.801870</td>\n",
       "      <td>0.889137</td>\n",
       "      <td>0.860441</td>\n",
       "      <td>0.851260</td>\n",
       "      <td>0.839475</td>\n",
       "      <td>0.802415</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.824589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.851562</td>\n",
       "      <td>0.850992</td>\n",
       "      <td>0.893639</td>\n",
       "      <td>0.883679</td>\n",
       "      <td>0.817092</td>\n",
       "      <td>0.894163</td>\n",
       "      <td>0.865981</td>\n",
       "      <td>0.832224</td>\n",
       "      <td>0.908533</td>\n",
       "      <td>0.840067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.878662</td>\n",
       "      <td>0.879877</td>\n",
       "      <td>0.872153</td>\n",
       "      <td>0.906954</td>\n",
       "      <td>0.883367</td>\n",
       "      <td>0.892846</td>\n",
       "      <td>0.901664</td>\n",
       "      <td>0.849126</td>\n",
       "      <td>0.824589</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29 rows √ó 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7   \\\n",
       "0   1.000000  0.897777  0.865769  0.927274  0.814605  0.849445  0.897746   \n",
       "1   0.897777  1.000000  0.886390  0.882154  0.864359  0.858079  0.926472   \n",
       "2   0.865769  0.886390  1.000000  0.896055  0.839453  0.868302  0.891039   \n",
       "3   0.927274  0.882154  0.896055  1.000000  0.814755  0.824425  0.892607   \n",
       "4   0.814605  0.864359  0.839453  0.814755  1.000000  0.788036  0.867569   \n",
       "5   0.849445  0.858079  0.868302  0.824425  0.788036  1.000000  0.855494   \n",
       "6   0.897746  0.926472  0.891039  0.892607  0.867569  0.855494  1.000000   \n",
       "7   0.899409  0.893035  0.851946  0.869157  0.789142  0.845567  0.870008   \n",
       "8   0.888698  0.868674  0.888574  0.885526  0.840695  0.894111  0.913901   \n",
       "9   0.843190  0.866818  0.828325  0.836706  0.767226  0.861864  0.851123   \n",
       "10  0.845380  0.840210  0.881216  0.826806  0.812519  0.912169  0.880090   \n",
       "11  0.907540  0.884934  0.875668  0.946640  0.838157  0.842756  0.895554   \n",
       "12  0.797256  0.757732  0.772453  0.841225  0.800958  0.710923  0.796526   \n",
       "13  0.815203  0.845144  0.817059  0.818404  0.828072  0.841017  0.834774   \n",
       "14  0.783988  0.780024  0.753832  0.735353  0.789667  0.736287  0.781332   \n",
       "15  0.886961  0.860154  0.856344  0.864299  0.795156  0.846405  0.880638   \n",
       "16  0.726719  0.711603  0.758791  0.714789  0.720214  0.822133  0.728669   \n",
       "17  0.822338  0.817655  0.838768  0.830438  0.818860  0.822453  0.849755   \n",
       "18  0.909160  0.882821  0.872007  0.893242  0.835657  0.834136  0.890722   \n",
       "19  0.885971  0.933510  0.862271  0.872986  0.876847  0.869659  0.931600   \n",
       "20  0.857606  0.861892  0.894841  0.856752  0.814360  0.887042  0.882980   \n",
       "21  0.893579  0.910783  0.904236  0.893440  0.839335  0.858907  0.895631   \n",
       "22  0.873972  0.848377  0.840616  0.848497  0.789835  0.880448  0.875633   \n",
       "23  0.980201  0.902980  0.878080  0.942968  0.831192  0.869259  0.916188   \n",
       "24  0.883580  0.877640  0.859450  0.874131  0.813671  0.851684  0.878453   \n",
       "25  0.829955  0.813705  0.898116  0.866142  0.846855  0.855571  0.860260   \n",
       "26  0.889992  0.894445  0.866050  0.870952  0.769716  0.879893  0.885978   \n",
       "27  0.852783  0.823056  0.841487  0.836712  0.802049  0.838531  0.849385   \n",
       "28  0.851562  0.850992  0.893639  0.883679  0.817092  0.894163  0.865981   \n",
       "\n",
       "          8         9         10  ...        20        21        22        23  \\\n",
       "0   0.899409  0.888698  0.843190  ...  0.885971  0.857606  0.893579  0.873972   \n",
       "1   0.893035  0.868674  0.866818  ...  0.933510  0.861892  0.910783  0.848377   \n",
       "2   0.851946  0.888574  0.828325  ...  0.862271  0.894841  0.904236  0.840616   \n",
       "3   0.869157  0.885526  0.836706  ...  0.872986  0.856752  0.893440  0.848497   \n",
       "4   0.789142  0.840695  0.767226  ...  0.876847  0.814360  0.839335  0.789835   \n",
       "5   0.845567  0.894111  0.861864  ...  0.869659  0.887042  0.858907  0.880448   \n",
       "6   0.870008  0.913901  0.851123  ...  0.931600  0.882980  0.895631  0.875633   \n",
       "7   1.000000  0.852274  0.880922  ...  0.874532  0.896833  0.946425  0.808719   \n",
       "8   0.852274  1.000000  0.849303  ...  0.892467  0.884985  0.873981  0.911276   \n",
       "9   0.880922  0.849303  1.000000  ...  0.872618  0.839869  0.849620  0.791084   \n",
       "10  0.854255  0.901191  0.832143  ...  0.870869  0.877538  0.869662  0.888018   \n",
       "11  0.856726  0.912837  0.848631  ...  0.907210  0.862158  0.886034  0.867450   \n",
       "12  0.753601  0.797316  0.680390  ...  0.766723  0.782304  0.798825  0.746512   \n",
       "13  0.827190  0.816113  0.817153  ...  0.859192  0.833213  0.836468  0.768343   \n",
       "14  0.750503  0.747724  0.666582  ...  0.749754  0.733695  0.781053  0.722573   \n",
       "15  0.840610  0.879507  0.811685  ...  0.857478  0.870504  0.878393  0.918057   \n",
       "16  0.685988  0.813223  0.702363  ...  0.733064  0.764451  0.711773  0.815616   \n",
       "17  0.793364  0.862826  0.796336  ...  0.847940  0.802369  0.825285  0.831318   \n",
       "18  0.801153  0.913530  0.812816  ...  0.908361  0.833665  0.849931  0.882858   \n",
       "19  0.874532  0.892467  0.872618  ...  1.000000  0.870740  0.892927  0.866311   \n",
       "20  0.896833  0.884985  0.839869  ...  0.870740  1.000000  0.921079  0.848599   \n",
       "21  0.946425  0.873981  0.849620  ...  0.892927  0.921079  1.000000  0.847019   \n",
       "22  0.808719  0.911276  0.791084  ...  0.866311  0.848599  0.847019  1.000000   \n",
       "23  0.899147  0.909959  0.866520  ...  0.903511  0.864922  0.899236  0.889096   \n",
       "24  0.838027  0.889372  0.794668  ...  0.873769  0.863425  0.867141  0.902166   \n",
       "25  0.786266  0.905538  0.767258  ...  0.852776  0.856764  0.850989  0.870294   \n",
       "26  0.880878  0.867178  0.861310  ...  0.901577  0.858994  0.877794  0.850381   \n",
       "27  0.755726  0.893887  0.751409  ...  0.807337  0.817670  0.801870  0.889137   \n",
       "28  0.832224  0.908533  0.840067  ...  0.878662  0.879877  0.872153  0.906954   \n",
       "\n",
       "          24        25        26        27        28        29  \n",
       "0   0.980201  0.883580  0.829955  0.889992  0.852783  0.851562  \n",
       "1   0.902980  0.877640  0.813705  0.894445  0.823056  0.850992  \n",
       "2   0.878080  0.859450  0.898116  0.866050  0.841487  0.893639  \n",
       "3   0.942968  0.874131  0.866142  0.870952  0.836712  0.883679  \n",
       "4   0.831192  0.813671  0.846855  0.769716  0.802049  0.817092  \n",
       "5   0.869259  0.851684  0.855571  0.879893  0.838531  0.894163  \n",
       "6   0.916188  0.878453  0.860260  0.885978  0.849385  0.865981  \n",
       "7   0.899147  0.838027  0.786266  0.880878  0.755726  0.832224  \n",
       "8   0.909959  0.889372  0.905538  0.867178  0.893887  0.908533  \n",
       "9   0.866520  0.794668  0.767258  0.861310  0.751409  0.840067  \n",
       "10  0.869199  0.853123  0.893081  0.865068  0.818603  0.893788  \n",
       "11  0.934163  0.874962  0.875416  0.881039  0.841396  0.916184  \n",
       "12  0.812341  0.740174  0.788530  0.737813  0.763949  0.785362  \n",
       "13  0.833017  0.783124  0.811284  0.818609  0.747984  0.828682  \n",
       "14  0.777644  0.693600  0.746741  0.721366  0.724773  0.686771  \n",
       "15  0.892985  0.878297  0.850727  0.862710  0.882088  0.864454  \n",
       "16  0.743748  0.793567  0.812558  0.727605  0.797270  0.834660  \n",
       "17  0.845693  0.843497  0.901613  0.803563  0.784669  0.850865  \n",
       "18  0.921141  0.890950  0.882847  0.874082  0.862578  0.873536  \n",
       "19  0.903511  0.873769  0.852776  0.901577  0.807337  0.878662  \n",
       "20  0.864922  0.863425  0.856764  0.858994  0.817670  0.879877  \n",
       "21  0.899236  0.867141  0.850989  0.877794  0.801870  0.872153  \n",
       "22  0.889096  0.902166  0.870294  0.850381  0.889137  0.906954  \n",
       "23  1.000000  0.893366  0.855512  0.894610  0.860441  0.883367  \n",
       "24  0.893366  1.000000  0.858422  0.855367  0.851260  0.892846  \n",
       "25  0.855512  0.858422  1.000000  0.824256  0.839475  0.901664  \n",
       "26  0.894610  0.855367  0.824256  1.000000  0.802415  0.849126  \n",
       "27  0.860441  0.851260  0.839475  0.802415  1.000000  0.824589  \n",
       "28  0.883367  0.892846  0.901664  0.849126  0.824589  1.000000  \n",
       "\n",
       "[29 rows x 29 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_df_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
